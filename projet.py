import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import math
import pandas as pd

points =[(1,1),(1,2),(1,5),(3,4),(4,3),(6,2),(0,4)]
noms= ["M1", "M2", "M3", "M4", "M5", "M6", "M7"]

#########################################
#               Partie 1                #
#########################################

print("\nPARTIE 1\n")

#S√©parer les coordonn√©es x et y
x = np.array([p[0] for p in points])
y = np.array([p[1] for p in points])


def afficher_stats(valeurs, noms):
    print("Moyenne :", np.mean(valeurs))
    print("M√©diane :", np.median(valeurs))
    print("Variance :", np.var(valeurs))
    print("Ecart-type :", np.std(valeurs))
    print("Minimum :", np.min(valeurs))
    print("Maximum :", np.max(valeurs))
    print("Etendue :", np.ptp(valeurs))


afficher_stats(x, "x")
afficher_stats(y, "y")

plt.scatter(x, y, color='blue')
for i in range(len(points)):
    plt.text(x[i] + 0.1, y[i] + 0.1, noms[i])
plt.title("Nuage de points")
plt.xlabel("x")
plt.ylabel("y")
plt.grid()
plt.show()

#   Interpr√©tation statistique :

#Les valeurs xx sont plus dispers√©es que les valeurs yy.
#La moyenne de xx est plus faible que celle de yy, indiquant une asym√©trie spatiale.
#L‚Äô√©tendue plus grande pour xx traduit une plus grande variabilit√© sur l‚Äôaxe horizontal.

#   Interpr√©tation graphique :

#Le nuage montre une certaine tendance croissante mais non strictement lin√©aire.
#Il est possible qu‚Äôune relation lin√©aire approximative existe entre xx et yy, √† tester dans la partie suivante.


#########################################
#               Partie 2                #
#########################################

print("\nPARTIE 2\n")

# 2.1 Calcul des Coefficients de R√©gression

# Moyennes
x_moy = np.mean(x)
y_moy = np.mean(y)

# Coefficients
b1 = np.sum((x - x_moy) * (y - y_moy)) / np.sum((x - x_moy)**2)
b0 = y_moy - b1 * x_moy

print("bo =", b0)
print("b1 =", b1)

# Mod√®le de pr√©dictions
y_pred = b0 + b1 * x


# 2.3 Coefficient de D√©termination R¬≤

# R¬≤
SCE = np.sum((y - y_pred) ** 2)
SCT = np.sum((y - y_moy) ** 2)
SCR = np.sum((y_pred - y_moy) ** 2)
R2 = 1 - SCE / SCT
print("R¬≤ (Coefficient de d√©termination) :", R2)


# 2.2 Visualisation de la Droite de R√©gression

# Affichage
plt.scatter(x, y, label='Points')
plt.plot(x, y_pred, color='red', label=f'Droite de r√©gression: y = {b0:.2f} + {b1:.2f}x')
plt.legend()
plt.title(f'Droite de r√©gression lin√©aire simple (R¬≤ = {R2:.2f})')
plt.xlabel("x")
plt.ylabel("y")
plt.grid(True)
plt.show()
plt.savefig("figure_2_1.jpg")
print("Graphique enregistr√© dans le fichier figure_2_1.jpg")


#########################################
#               Partie 3                #
#########################################

print("\nPARTIE 3\n")

#1. R√©sidus et somme des carr√©s des erreurs (SCE)
e = y - y_pred

SCE=0

for i in range(len(points)):
    SCE += e[i]**2 
print("SCE (Somme des carr√©s des erreurs) :", SCE)

#2. Etimation de la variance des erreurs : MSE
n= len(x)

MSE = SCE / (n-2)

print("MSE (Erreur quadratique moyenne) :", MSE)

#Le n-2 vient du fait qu'on a estim√© 2 param√®tres (b0 et b1). On parle alors de degr√©s de libert√©.

#3. Ecart-type des erreurs
s = np.sqrt(MSE)

print("√âcart-type des erreurs :", s)

#4. Interpr√©tation des R√©sultats

# üîπ Interpr√©tation des coefficients

#     b0=3.33 (ordonn√©e √† l'origine) : c'est la valeur estim√©e de y quand x=0. Cela signifie qu'√† l'origine de l'axe x, la droite de r√©gression pr√©voit y=3.33.

#     b1=‚àí0.15 (pente) : chaque augmentation de 1 unit√© en x entra√Æne une baisse moyenne de y de 0.15. La relation est donc l√©g√®rement d√©croissante, mais tr√®s faible.

# üîπ Coefficient de d√©termination R2

#     Le R2=0.049 (soit 4.9%) indique que seulement 4.9% de la variation de y est expliqu√©e par la variable x.

#     Cela signifie que la droite de r√©gression explique tr√®s peu la variabilit√© des points. La majorit√© de la variation de y provient donc d'autres facteurs non captur√©s par ce mod√®le.

# üîπ Analyse des erreurs

#     SCE (Somme des carr√©s des erreurs) : 11.42
#     ‚Üí mesure l‚Äôerreur globale du mod√®le (plus elle est faible, meilleur est l‚Äôajustement).

#     MSE (Erreur quadratique moyenne) : 2.28
#     ‚Üí estimation de la variance des erreurs r√©siduelles.

#     √âcart-type des erreurs : 1.51
#     ‚Üí en moyenne, les pr√©dictions du mod√®le s‚Äô√©cartent de 1.51 unit√©s des valeurs r√©elles.

#     Compar√© √† l‚Äô√©cart-type total de y qui est de 1.31, cela montre que la droite n'am√©liore pas vraiment la pr√©diction par rapport √† une moyenne constante.

#########################################
#               Partie 4                #
#########################################

print("\nPARTIE 4\n")

# 4.1 Estimation de la variance des erreurs

# 4.2 Erreurs Standards des Coefficients

# Erreur standard de la pente
somme_x=0
for i in range(len(points)):
    somme_x += (x[i]-x_moy)**2

SEb1 = s/np.sqrt(somme_x)
print("Erreur standard de la pente (SEb1):", SEb1)

# Erreur standard de l'ordonn√©e √† l'origine
SEb0 = s*np.sqrt(1/n+x_moy**2/somme_x)
print("Erreur standard de l'ordonn√©e √† l'origine (SEb0)", SEb0)
print("\n")

# 4.3 Test d'Hypoth√®se pour la Pente (b1)

# Hypoth√®ses :
# H0 : b1 = 0 (pas de relation lin√©aire)
# H1 : b1 != 0 (relation lin√©aire significative)

# Statistique de test

t_b1 = (b1-0)/SEb1 # t suit une loi de Student donc on calcul la p-valeur pour savoir si on rejette l'hypoth√®se ou non
p_b1 = 2 * (1 - stats.t.cdf(abs(t_b1), df=n - 2))
print("b1 :")
print("valeur de test", t_b1)
print("p-valeur:", p_b1)
if(p_b1<0.05):
    print("p-valeur<0,05 donc on rejette l'hypoth√®se, b1 est significatif")
else:
    print("p-valeur>0,05 donc on ne rejette pas l'hypoth√®se, il n'y a pas de preuve de relation lin√©aire")
print("b1 = ", b1)
print("\n")

# 4.4 Test d'Hypoth√®se pour l'Ordonn√©e √† l'Origine (b0)
# Hypoth√®ses : H0 : b0 = 0 vs H1 : b0 != 0

# Statistique de test

t_b0 = (b0-0)/SEb0
p_b0 = 2 * (1 - stats.t.cdf(abs(t_b0), df=n - 2))
print("b0 :")
print("valeur de test", t_b0)
print("p-valeur:", p_b0)
if(p_b0<0.05):
    print("p-valeur<0,05 donc on rejette l'hypoth√®se, b0 est significatif, la constante n'est pas nulle")
else:
    print("p-valeur>0,05 donc on ne rejette pas l'hypoth√®se, il n'y a pas de preuve de relation lin√©aire, b0 n'est pas significatif")
print("b0 = ", b0)
print("\n")

# 4.5 Intervalles de Confiance pour les Coefficients

alpha = 0.05

t_crit = stats.t.ppf(1-alpha/2, df= n-2)

IC_b1=[float(b1-t_crit*SEb1),float(b1+t_crit*SEb1)]
IC_b0=[float(b0-t_crit*SEb0),float(b0+t_crit*SEb0)]

print("Intervalle de confiance √† 95% pour b1:", IC_b1)
print("Intervalle de confiance √† 95% pour b0:", IC_b0)

# 4.6 Interpr√©tation des Tests Statistiques

# Pour l'erreur standard des coefficients on a :
#   Erreur standard de la pente SEb1 = 0.2885
#   Erreur standard √† l'ordonn√©e √† l'origine SEb0 = 0.8724

# Test d'hypoth√®se pour la pente (b1) :
#   Hypoth√®ses :
#        H0 : b1 = 0 (pas de relation lin√©aire)
#        H1 : b1 != 0 (relation lin√©aire significative)
#   Statistique de test t : -0.5054
#   p-valeur : 0.6347 (> 0.05)
#   Conclusion :
#       On ne rejette pas H0.
#       Il n‚Äôy a pas de preuve statistique d‚Äôune relation lin√©aire significative entre les variables.
#       Autrement dit, la pente n‚Äôest pas significative.

# Test d'hypoth√®se pour l'ordonn√©e √† l'origine (b0) :
#   Hypoth√®ses :
#        H0 : b0 = 0 (la constante est nulle)
#        H1 : b0 != 0 (la constante est significativement diff√©rente de 0)
#   Statistique de test t : 3.8208
#   p-valeur : 0.0124 (< 0.05)
#   Conclusion :
#       On rejette H0.
#       La constante est significativement diff√©rente de 0.

# Intervalle de confiance √† 95% :
#   Pour b1 (la pente):
#       [-0.8875, 0.5958]
#       Contient z√©ro donc coh√©rent avec le fait que b1 n‚Äôest pas significatif.
#   Pour b0 (l'ordonn√©e √† l'origine) :
#       [1.0907, 5.5760]
#       Ne contient pas z√©ro donc confirme que b0 est significatif.

# Interpr√©tation finale : 
#   La constante b0 est significative : le mod√®le pr√©dit une valeur moyenne de y autour de 3.33 m√™me quand x = 0.
#   La pente b1 n‚Äôest pas significative : la variable explicative x n‚Äôexplique pas de mani√®re significative la variation de y dans notre √©chantillon.
#   On pourrait envisager d'autres mod√®les, transformations de variables, ou d‚Äôaugmenter la taille de l‚Äô√©chantillon si on soup√ßonne une relation qui n‚Äôappara√Æt pas ici.

#########################################
#               Partie 5                #
#########################################

print("\nPARTIE 5\n")

# Classification Ascendante Hi√©rarchique (CAH)

# 1.a
def dist(p1, p2):
    """Distance euclidienne"""
    distance = math.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)
    print("La distance euclidienne vaut : ", distance)
    return distance

# 1.b
def dist1(p1, p2):
    """Distance de Manhattan"""
    distance = abs(p1[0]-p2[0]) + abs(p1[1]-p2[1])
    print("La distance de Manhattan vaut : ", distance)
    return distance

# 1.c
def dist_inf(p1, p2):
    """Distance de Chebyshev (max)"""
    distance = max(abs(p1[0]-p2[0]), abs(p1[1]-p2[1]))
    print("La distance de Chebyshev vaut : ", distance)
    return distance

# 1.d : Distance de Ward ???


# 2.
'''
La fonction dist_min est une fonction permettant de retourner un couple de point situ√©s √† une distance minimale l'un de l'autre
param√®tre
    tableau : un tableau de points
    dist_func : la fonction du calcul de la distance entre deux points que l'on veut utiliser
'''
def dist_min(tableau, dist_func):
    min_d = float('inf')
    couple_points = None
    for i in range(len(tableau)):
        for j in range(i+1, len(tableau)):
            d = dist_func(tableau[i], tableau[j])
            if d < min_d:
                min_d = d
                couple_points = (tableau[i], tableau[j])
    return couple_points, min_d


# 3.

# Trac√©
plt.scatter(x, y, color='blue')
for i in range(len(points)):
    plt.text(x[i] + 0.1, y[i] + 0.1, noms[i])
plt.title("Nuage de points")
plt.xlabel("x")
plt.ylabel("y")
plt.grid()
plt.show()

# Initialisation de la matrice
n = len(x)
matrice_1 = np.zeros((n, n))

# Remplissage de la matrice avec d¬≤
for i in range(n):
    for j in range(n):
        xi, yi = points[i]
        xj, yj = points[j]
        d_squared = (xi - xj)**2 + (yi - yj)**2
        matrice_1[i][j] = d_squared

# Affichage avec pandas pour lisibilit√©
data = pd.DataFrame(matrice_1, index=x, columns=x)
print("Matrice des distances euclidiennes au carr√© :\n")
print(data.round(1))

'''
# Encadrer M1 et M7 (Classe Œì‚ÇÅ)
x_vals = [points[0][0], points[6][0]]
y_vals = [points[0][1], points[6][1]]
#plt.plot(x_vals, y_vals, 'ro--')
plt.scatter(x_vals, y_vals, color='red')
plt.title("Regroupement initial : Classe Œì‚ÇÅ = {M1, M7}")
plt.grid(True)
plt.xlim(-1, 7)
plt.ylim(0, 6)
plt.show()

plt.savefig("figure_5_3.jpg")
print("Graphique enregistr√© dans le fichier figure_5_3.jpg")
'''

#4.

# Trouver le couple le plus proche
min_dist = float('inf') #valeur tr√®s grande (infini)
min_pair = (0, 0) 

min_pair, min_dist = dist_min(points, dist)

# Points de Gamma1 (groupe initial)
# dist_min retourne un couple de points (p1, p2)
p1, p2 = min_pair

# retrouver les indices de ces points dans la liste points
i = points.index(p1)
j = points.index(p2)

Gamma1 = [points[i], points[j]]
Gamma1_nom = "G1"

# Points restants
reste_points = []
reste_noms = []
for k in range(n):
    if k != i and k != j:
        reste_points.append(points[k])
        reste_noms.append(noms[k])

# Fonction : distance entre un point et un segment [A,B]
'''calcule la distance au carr√© entre un point P(px, py) et
un segment d√©fini par les points A(ax, ay) et B(bx, by)'''
def distance_point_segment(px, py, ax, ay, bx, by):
    #Composantes du vecteur AB
    ABx = bx - ax
    ABy = by - ay

    #Composantes du vecteur AP
    APx = px - ax
    APy = py - ay

    #Carr√© de la longueur du segment AB
    ab2 = ABx**2 + ABy**2
    if ab2 == 0: #si A et B sont confondus, alors c'est une distance avec un point
        return (px - ax)**2 + (py - ay)**2
    
    #Projection orthogonale
    t = (APx * ABx + APy * ABy) / ab2
    if t < 0: #la projection est avant A donc A est le plus proche
        closest_x, closest_y = ax, ay
    elif t > 1: #la projection est apr√®s B donc B est le plus proche
        closest_x, closest_y = bx, by
    else:
        closest_x = ax + t * ABx
        closest_y = ay + t * ABy

    #On retourne la distance au carr√© entre P et le point le plus proche du segment AB
    dx = px - closest_x
    dy = py - closest_y

    return dx**2 +dy**2

# Matrice 6x6 : Gamma1 + 5 autres points
noms_total = [Gamma1_nom] + reste_noms
taille = len(noms_total)
matrice_2 = np.zeros((taille, taille))

for a in range(taille):
    for b in range(taille):
        if a == b: #la matrice est nulle sur la diagonale
            matrice_2[a][b] = 0
        elif a == 0: #distance entre le point et Gamma 1 (distance segment-point)
            px, py = reste_points[b - 1]
            ax, ay = Gamma1[0]
            bx, by = Gamma1[1]
            matrice_2[a][b] = distance_point_segment(px, py, ax, ay, bx, by)
        elif b == 0: #distance entre Gamma 1 et le point (distance segment-point)
            px, py = reste_points[a - 1]
            ax, ay = Gamma1[0]
            bx, by = Gamma1[1]
            matrice_2[a][b] = distance_point_segment(px, py, ax, ay, bx, by)
        else: #distance entre deux points
            x1, y1 = reste_points[a - 1]
            x2, y2 = reste_points[b - 1]
            matrice_2[a][b] = (x1 - x2)**2 + (y1 - y2)**2

# Affichage final
print("Matrice avec Gamma1 et les 5 autres points (distances au carr√©) \n")
df = pd.DataFrame(matrice_2, index=noms_total, columns=noms_total)
print(df.round(1))
